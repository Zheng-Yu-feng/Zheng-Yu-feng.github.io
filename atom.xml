<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yufeng&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/2789dc0497448f0e77e0bc6edfcb5435</icon>
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-12-20T09:10:54.114Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Zheng Yufeng</name>
    <email>3054661346@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>local_llm（详细设计）</title>
    <link href="http://example.com/2025/12/20/local-llm2/"/>
    <id>http://example.com/2025/12/20/local-llm2/</id>
    <published>2025-12-20T08:05:15.000Z</published>
    <updated>2025-12-20T09:10:54.114Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><p><strong>项目背景</strong>：在数据安全与隐私保护成为企业数字化核心关切的时代，公有云AI服务因其在敏感数据合规性、网络依赖性及定制灵活性方面的局限，难以满足高安全隔离场景的需求。为此，本项目旨在设计并实现一套支持完全私有化部署、开箱即用的大语言模型交互工具系统。该系统允许用户在离线或隔离环境中，安全、自主地进行智能对话、文档分析与知识问答，从根本上满足企业“数据不出域、模型可自选、部署可迁移”的核心诉求，并确保在Windows与Linux平台中的稳定集成与高效运行。</p><p><strong>项目范围</strong>：本项目将交付一个功能完整、支持跨平台运行的大语言模型交互工具，具体涵盖以下核心模块与功能：</p><p>（1）灵活输入与交互界面：提供生成式智能问答会话窗口，支持纯文本输入与文档上传（支持TXT、MD格式）两种输入方式，实现多模态交互。</p><p>（2）跨平台与部署支持：支持在Windows与Linux操作系统环境中通过API进行集成与调用。</p><p>（3）私有化部署：提供标准化私有化部署方案，便于迁移至不同服务平台。</p><p>（4）多模型集成与管理：集成6种参数规模≥7B的大语言模型（Qwen、Baichuan、Yi、Mistral、Zephyr、九格）。支持API调用，用户可通过有效密钥调用模型，兼容公有模型服务。</p><p>（5）输出控制与参数调节：提供采样策略（如temperature、top_p、top_k）与生成长度（max_length）等可调节参数界面，实现对模型输出内容与行为的精细化控制。</p><p>（6）响应性能记录与监控：具备模型响应时间与输出速度的记录功能，支持性能跟踪与优化分析。</p><p>（7）用户界面与体验：基于Vue 3与Element Plus构建现代化、响应式的图形界面，确保交互流程直观、操作简单易懂。</p><p>（8）多会话上下文管理：支持创建、保存、切换多个独立对话会话，确保各会话上下文隔离且持久化存储。</p><p>（9）对话历史与导出：完整记录所有对话历史，支持以Markdown或JSON格式导出，便于审计与知识留存。</p><p>（10）系统可维护性与可升级性：采用标准化开发与部署框架，保障系统具备良好的可维护性与可扩展性，支持快速故障修复与平滑版本升级。</p><h1 id="二、总体设计概述"><a href="#二、总体设计概述" class="headerlink" title="二、总体设计概述"></a>二、总体设计概述</h1><h2 id="2-1-系统架构"><a href="#2-1-系统架构" class="headerlink" title="2.1 系统架构"></a>2.1 系统架构</h2><p>本系统采用前后端分离的单体应用架构，所有组件运行在同一个用户进程中。前端是使用现代Vue 3技术栈构建的单页面应用（SPA），后端是Python Flask服务器，两者通过RESTful API进行通信。本系统采用清晰的分层架构设计，旨在实现关注点分离、模块化以及良好的可维护性。整体架构自上而下可分为表现层、Web服务层、业务逻辑层与大语言模型层，各层通过定义良好的接口进行通信，共同协作以提供完整的私有化大语言模型交互体验：</p><p><img src="/../image/clip_image002.jpg" alt="img"></p><p>图1 系统架构图</p><p><strong>表现层</strong>：作为用户直接交互的界面，承担着渲染视图、管理用户状态与处理交互事件的核心职责。该层基于现代前端技术栈构建：使用 Vue 3 框架及其响应式系统（如 reactive）与组合式 API（Composition API）来构建高度动态和响应式的用户界面组件。Vue Router 负责管理整个单页面应用（SPA）的路由导航，定义了诸如 &#x2F;login（登录）、&#x2F;chat（主聊天界面）、&#x2F;set_model（模型选择）、&#x2F;change_parameters（参数调整）、&#x2F;upload（文件上传）等一系列路由路径，确保用户操作能无缝切换到对应的功能视图。应用状态由 Pinia 进行集中式管理，其存储（Store）中维护着包括当前用户认证状态、所有聊天会话的列表及其激活状态、每个会话内完整的消息历史记录、系统当前可用的大语言模型列表与用户选中的模型配置、以及各类模型采样参数（如 temperature, top_p 等）在内的全局状态。所有与后端服务的 HTTP 通信均通过 Axios 库完成，它负责发送 RESTful API 请求并处理响应。界面组件主要采用 Element Plus 组件库进行构建，以快速实现美观、一致且易于使用的聊天界面、表单、弹窗等元素。此外，Nanoid 库被用于在前端生成唯一标识符（如会话 ID、消息 ID），保证客户端数据标识的独立性与唯一性。</p><p><strong>Web</strong> <strong>服务器层</strong>: 充当了前端应用与后端业务逻辑之间的桥梁和静态资源托管者。该层以 Flask 轻量级 Web 框架为核心实现。其主要职责之一是托管由 Vue 3 项目构建后产生的静态文件（包括 HTML、JavaScript 和 CSS），为前端 SPA 提供服务。同时，Flask 应用实现了一套完整的 RESTful API 接口（例如 &#x2F;api&#x2F;login, &#x2F;api&#x2F;chat, &#x2F;api&#x2F;models 等），供前端 Axios 调用。此层接收前端的请求，进行初步的验证与转发，并将业务逻辑层的处理结果封装成 JSON 响应返回给前端。为了提升会话上下文管理的性能与持久化能力，该层集成了 Redis 作为缓存数据库，主要用于存储用户的历史对话消息。当用户进行连续对话时，系统可以从 Redis 中快速读取历史上下文，并将其组合到新的提示词中，从而使得大语言模型能够基于完整的对话历史进行连贯、准确的回答。</p><p><strong>业务逻辑层</strong>:是应用程序的核心，封装了所有关键的业务规则与处理流程。它独立于 Web 框架的具体实现，包含一系列协调不同功能的处理器或服务模块。认证管理模块负责校验用户通过前端提交的登录凭证，确保系统访问的安全性。会话管理模块在服务端内存中维护着所有活跃聊天会话的数据结构，每个会话对象不仅包含其元信息，还关联着完整的消息历史链。消息处理模块负责接收来自前端的用户输入（无论是纯文本还是经由文档解析后的文本内容），并依据当前会话 ID 从 Redis 中获取历史消息，随后将当前 query 与历史上下文智能地组合、拼接成符合特定大语言模型要求的提示（Prompt）格式。模型调用器（Model Invoker）是本层的一个关键组件，它对外提供统一的调用接口（如 generate(prompt, parameters)），业务逻辑通过此接口与底层的大语言模型层交互，从而屏蔽了不同模型在加载、推理细节上的差异。此外，导出逻辑模块负责将用户指定的会话历史，按照要求转换为结构化的 Markdown 文档或 JSON 数据格式，并生成可供下载的文件。</p><p><strong>大语言模型层</strong>: 是系统的智能引擎，直接负责文本的生成任务。该层基于 Hugging Face Transformers 库构建，负责加载、管理在本地面部部署的多个大型语言模型（如 Qwen、Baichuan、Yi、Mistral、Zephyr、九格等）。鉴于大模型对内存和显存资源消耗巨大，本层实现了一套精细的模型缓存与生命周期管理策略。这包括：在应用启动时预加载一个或多个最常用的默认模型以加快首次响应速度；支持按需加载，即当用户在前端切换模型时，再动态加载对应的模型至内存；以及基于最近最少使用（LRU） 算法的缓存管理，在设定的内存阈值下，自动卸载最久未被使用的模型，以维持系统整体的资源平衡。该层对外提供统一的文本生成函数，能够接收来自业务逻辑层的提示文本和各种生成参数（如 temperature、top_p、top_k、max_length 等），并调用相应模型进行推理，最后将生成的文本结果返回。</p><h2 id="2-2技术栈"><a href="#2-2技术栈" class="headerlink" title="2.2技术栈"></a>2.2技术栈</h2><p>在技术栈的选择上，本项目充分考虑了功能性、性能、开发效率以及私有化部署的要求。</p><p>后端技术栈以 Python 3.10+ 为主要开发语言，因其在人工智能和数据处理领域的丰富生态。Web 服务框架选用 Flask，其轻量、灵活的特性非常适合构建此类以 API 为核心的中间层服务，便于快速开发和维护 RESTful 接口。模型推理的核心依赖于 Hugging Face Transformers 库，并搭配 PyTorch 或 TensorFlow 作为底层计算框架，这为加载和运行各种开源大语言模型提供了标准化支持。为处理用户上传的多种格式文档（如txt、md），集成了对应文档解析库，用于提取纯文本内容。数据序列化主要使用 Python 原生的 json 模块。为高效管理对话状态，选择 Redis 作为高速缓存数据库，存储会话历史，确保上下文快速存取。</p><p>前端技术栈围绕构建现代化、高性能的单页面应用展开。选用 Vue 3 作为核心框架，并主要使用其组合式 API（Composition API） 进行开发，以更好地组织复杂组件的逻辑并提高代码的可复用性。应用状态由 Pinia 管理，它提供了比 Vuex 更简洁、类型友好的 API。路由控制由 Vue Router 处理，以实现多视图间的无缝跳转。用户界面基于 Element Plus（适配 Vue 3 的版本）进行构建，大幅提升了开发效率并保证了视觉一致性。所有 HTTP 请求通过 Axios 发起，它提供了强大的拦截器、错误处理等功能。唯一 ID 的生成使用轻量的 Nanoid 库。整个前端项目使用 Vite 作为构建工具与开发服务器，它提供了极快的冷启动和模块热更新（HMR）体验。开发语言采用 TypeScript，通过静态类型检查来增强代码的健壮性和可维护性。</p><p>模型管理策略是项目成功的关键。所有集成的模型均需转换为或本身即为 Hugging Face Transformers 库支持的格式，确保加载接口的统一。初始支持的模型列表包括 Qwen, Baichuan, Yi, Mistral, Zephyr, 九格等，并保留了扩展接口。模型缓存策略的设计紧密结合硬件资源（GPU&#x2F;CPU 内存），动态管理模型的加载与卸载，在用户体验（减少等待）与资源占用之间取得平衡。</p><p>数据流与存储方案遵循简洁高效的原则。前后端之间通过基于 JSON 格式的 HTTP（RESTful） 协议进行通信。运行时的大部分动态数据（如前端状态、后端会话对象）存储在内存中，分别由前端的 Pinia Store 和后端的 Python 数据结构进行管理。对于数据的持久化，主要依赖于用户主动触发的导出操作，将对话历史保存为 Markdown 或 JSON 文件至本地磁盘，而非在服务端进行自动化的数据库持久化，这符合工具类应用及隐私保护的设计初衷。</p><p>开发与部署流程采用前后端分离的模式。在开发阶段，前端 Vite 开发服务器通过配置代理将 API 请求转发至后端的 Flask 开发服务器，方便并行开发和调试。构建时，前端代码通过 vue build（或 vite build）命令编译成静态文件。随后，这些静态文件被复制到 Flask 应用指定的静态文件目录中，形成可独立部署的整体。为达成“开箱即用”的私有化部署目标，最终封装方案考虑使用 PyInstaller 将整个 Python 后端及其依赖、前端静态资源打包成一个独立的可执行文件；同时，也支持使用 Docker 进行容器化部署，以增强环境一致性和迁移便利性。应用启动后，将自动加载模型、启动内嵌的 Flask Web 服务器，并可选择自动打开系统默认浏览器导航至本地应用地址，为用户提供一体化的启动体验。</p><h2 id="2-3-关键设计考虑"><a href="#2-3-关键设计考虑" class="headerlink" title="2.3 关键设计考虑"></a>2.3 关键设计考虑</h2><p>在系统设计过程中，以下几个关键问题需要特别关注，并制定了相应的解决方案：</p><p>内存与模型资源管理是首要挑战。大型语言模型参数庞大，同时加载多个模型极易耗尽系统内存。为此，我们设计了动态的模型加载策略：结合“预加载”、“按需加载”和“LRU缓存淘汰”机制。该策略的核心在于精细化的生命周期控制。具体而言，当用户在前端界面切换模型时，后端业务逻辑层会首先检查目标模型是否已加载在内存中。若未加载，则从磁盘加载；若已加载，则直接激活。至关重要的优化环节发生在模型卸载时：系统会显式地释放当前已加载模型实例所占用的资源。这通过执行del self.current_instance来删除Python对象引用，并紧接着调用torch.cuda.empty_cache()来强制清空PyTorch在CUDA设备上占用的、现已不再引用的缓存内存。这一组合操作能即时、有效地回收珍贵的GPU显存，为加载下一个模型腾出空间，从而在有限的硬件资源下支持多个大型模型的动态切换。在用户界面中，当模型处于加载、卸载或切换状态时，会提供明确的视觉反馈（如加载指示器、进度提示），让用户感知当前系统状态，提升操作的可预期性与流畅度。</p><p>前后端状态同步是保证应用一致性的基础。前端以 Pinia Store 作为状态的“单一数据源”，所有组件都从中读取状态。而后端业务逻辑层也维护着相应的会话与消息状态。两者之间通过严谨的 API 调用契约保持同步：前端的每一个可能改变状态的操作（如发送消息、切换模型）都会触发一个 API 调用，后端处理成功后，会在响应中返回最新的、权威的状态数据（或至少返回成功标识），前端随后根据响应结果更新本地的 Pinia Store，从而确保两端状态的一致。</p><p>文件上传与内容处理流程需要完整的设计。流程始于前端通过 &lt; input type&#x3D;”file” &gt; 元素选择文件并上传至后端 &#x2F;api&#x2F;upload 接口。后端接收文件后，根据其扩展名（如 .md, .txt）分发给相应的文档解析库进行文本内容提取。提取出的纯文本经过必要的清洗与格式化后，或直接作为一条用户消息放入会话上下文，或与用户提出的具体问题一同组合成新的提示词，最终提交给大语言模型进行处理和问答。</p><p>全面的错误处理机制是保障系统健壮性的关键。设计涵盖了从模型加载失败、文本生成过程中出现异常、到网络连接中断、用户输入无效、文件解析错误等各种可能的情况。后端 API 会捕获这些异常，并将其转化为结构化的错误信息（包含错误码和描述）返回给前端。前端 Axios 的拦截器会统一捕获 HTTP 错误和业务错误，并调用 Element Plus 的通知或消息框组件，向用户展示友好、明确的错误提示，指导用户进行后续操作。</p><p>后台推理任务与界面响应性是提升用户体验的重要方面。考虑到大语言模型推理可能是耗时较长的任务（数秒甚至数十秒），如果在Flask的主同步线程中直接执行生成调用，将会完全阻塞该工作进程，使其无法处理任何其他请求（如界面点击、历史查询），导致应用“卡死”。因此，我们将耗时的模型生成任务设计为异步后台任务。当用户发送一条消息后，后端API接口会立即接收请求并快速验证，随后将此生成任务提交至一个独立的后台线程或异步任务队列中执行，主线程则立即返回一个“任务已接收”的响应。前端据此可更新界面状态。这种异步化与解耦的设计确保了Web服务器主线程能够持续保持高响应速度，及时处理用户的其它交互操作，从而在整体上维持了应用的流畅性和可用性。</p><p>对话历史的完整性保障直接影响到多轮对话的质量。我们选择 Redis 作为历史消息的存储介质，主要是看中其高性能的内存读写特性，能够支持快速的上下文拼接。每次进行新的对话时，系统会从 Redis 中准确检索出该会话的所有历史消息，确保模型获得的上下文是完整且连续的。这种设计既满足了对话一致性的需求，又通过外部缓存减轻了业务逻辑层长期维护大量内存数据的压力。</p><h1 id="三、模块设计"><a href="#三、模块设计" class="headerlink" title="三、模块设计"></a>三、模块设计</h1><p>本章深入阐述系统核心功能模块的详细设计方案。基于总体架构，系统被分解为六个高内聚、低耦合的关键模块：认证、会话管理、聊天交互、模型管理、参数管理及用户界面。各模块通过明确的API接口契约进行通信，并通过共享的Redis缓存层（用于历史消息）和前端状态管理库（Pinia）维持数据的一致性。以下将逐一对各模块的职责、核心类&#x2F;组件设计、关键算法与逻辑流程、以及对外接口进行详细说明，旨在为开发实现提供清晰、无歧义的蓝图</p><h2 id="3-1认证模块"><a href="#3-1认证模块" class="headerlink" title="3.1认证模块"></a>3.1认证模块</h2><p><strong>功能描述</strong>：提供登录认证，用户输入正确的账号和密码方可进入系统</p><p><strong>实现细节</strong>：</p><p>l 前端用户输入账号和密码，点击登录按钮；</p><p>l 前端向后端请求 &#x2F;api&#x2F;login，发送账号和密码；</p><p>l 后端校验账号和密码；</p><p>l 如果匹配，则生成一个简单的会话 token返回前端。如果不匹配，则返回错误信息。</p><p>l 前端保存登录状态（存储在 localStorage）。</p><p>l 后续接口请求中，前端在请求头中附带 token，后端验证 token 有效性。</p><p><strong>接口设计：</strong></p><table><thead><tr><th>URL：POST &#x2F;api&#x2F;login</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>必填</td><td>说明</td></tr><tr><td>请求：</td><td>username</td><td>String</td><td>是</td><td>用户名</td></tr><tr><td></td><td>password</td><td>String</td><td>是</td><td>密码</td></tr><tr><td>响应：</td><td>message</td><td>String</td><td></td><td>登录结果提示信息</td></tr><tr><td></td><td>User_id</td><td>String</td><td></td><td>用户唯一标识，由后端生成并存储在Cookie中</td></tr></tbody></table><h2 id="3-2会话管理模块"><a href="#3-2会话管理模块" class="headerlink" title="3.2会话管理模块"></a>3.2会话管理模块</h2><p><strong>功能描述</strong>：支持多会话管理，用户可以创建、删除会话。每个会话独立保存聊天记录，用户可在不同会话之间自由切换，导出历史对话为 Markdown 或 JSON 文件。</p><p><strong>实现细节：</strong></p><p>（1）创建、删除会话：新建对话，openDialog()提示输入会话标题。点击确认（confirmNew()）后调用store.newConversation(title)创建新的会话。删除会话，调用store.deleteConversation(id)删除对应(id)的会话，若删除当前会话，自动切换到下一个可用会话。</p><p>（2）导出会话：用户点击“导出为Markdown”按钮，系统调用chatStore.exportCurrentConversationAsMarkdown()，将当前会话的消息整理为 Markdown 格式字符串，生成下载链接和文件名；用户点击“导出为JSON”按钮，系统调用chatStore.exportCurrentConversationAsJson()，将当前会话的消息序列化为 JSON 字符串, 生成下载链接和文件名；生成 Blob 数据并创建下载链接（URL.createObjectURL），动态创建一个不可见 标签，设置 href 和 download 属性。自动触发点击事件，完成文件下载。删除 标签，清理 DOM。</p><h2 id="3-3聊天交互模块"><a href="#3-3聊天交互模块" class="headerlink" title="3.3聊天交互模块"></a>3.3聊天交互模块</h2><p><strong>功能描述</strong>：提供生成式智能问答会话窗口，支持两种输入方式（文本\文档），调用LLM，接收问题并显示回复，并支持对响应及输出速度的记录。</p><p><strong>实现细节</strong>：</p><p>（1）智能问答：</p><p>l 前端构建用户信息，向后端请求&#x2F;api&#x2F;chat，携带对话信息;</p><p>l 后端接收请求，调用transformers库导入AutoModelForCausalLM, AutoTokenizer加载本地LLM，使用apply_chat_template()构建promp，合并history历史消息；</p><p>l 后端调用generate_reponse()执行模型生成[model.generate()或model.chat()]，返回响应response，将本次响应合并至history历史消息。</p><p>l 前端解析回复，计算耗时，更新UI，展示问答结果、响应时间，清空输入框，允许下一次输入。</p><p>（2）文档处理：</p><p>l 前端选择文件，支持TXT、MD格式，调用uploadFIle()，生成 FormData，通过 axios 发送到 &#x2F;api&#x2F;upload；</p><p>l 后端接收请求，检验格式，临时保存并读取文本text；</p><p>l 后端将text合并至history历史消息，返回文档加载成功响应；</p><p>l 前端接收响应，更新UI（显示文件名以及成功加载文档提示）。</p><p><strong>接口设计：</strong></p><table><thead><tr><th>URL：POST &#x2F;api&#x2F;chat</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>必填</td><td>说明</td></tr><tr><td>请求：</td><td>Messages</td><td>Array</td><td>是</td><td>历史消息列表，每个元素包含role(user&#x2F;assistant)和content</td></tr><tr><td></td><td>currentMessage</td><td>Object</td><td>是</td><td>当前用户消息对象，结构为{role:”user”,content:”…”}</td></tr><tr><td>响应：</td><td>Response</td><td>String</td><td></td><td>智能问答生成的回复内容</td></tr><tr><td></td><td>User_id</td><td>String</td><td></td><td>用户唯一标识，由后端生成并存储在Cookie中</td></tr></tbody></table><table><thead><tr><th>URL：POST &#x2F;api&#x2F;upload</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>必填</td><td>说明</td></tr><tr><td>请求：</td><td>File</td><td>File</td><td>是</td><td>需上传的文件，支持.txt和.md</td></tr><tr><td>响应：</td><td>Message</td><td>String</td><td></td><td>上传结果提示信息，“上传成功”</td></tr><tr><td></td><td>Text_preview</td><td>String</td><td></td><td>文件内容前200个字符的预览</td></tr><tr><td></td><td>User_id</td><td>String</td><td></td><td>用户唯一标识，由后端生成并存储在Cookie中</td></tr></tbody></table><h2 id="3-4模型管理模块"><a href="#3-4模型管理模块" class="headerlink" title="3.4模型管理模块"></a>3.4模型管理模块</h2><p><strong>功能描述</strong>：用户可在多个大语言模型（Qwen、Baichuan、Yi、Mistral、Zephyr、九格）之间切换，同时支持API调用模型，系统自动应用至后续问答。</p><p><strong>实现细节</strong>：</p><p>（1）加载模型</p><p>l 前端页面加载时，向后端请求&#x2F;api&#x2F;get_model；</p><p>l 后端接收请求&#x2F;api&#x2F;get_model，返回current_model；</p><p>l 前端调用 store.setModel(res.data.model) 将当前使用的模型写入全局状态。</p><p>（2）切换模型</p><p>l 切换模型触发onChange(val)，向后端请求&#x2F;api&#x2F;set_model；</p><p>l 后端接收请求&#x2F;api&#x2F;set_model，更新current_model[“model”]，调用chatbot.set_model(model_name) 让底层问答逻辑切换到对应模型。返回响应。如果选择API调用模型的方式，则需要使用正确的URL及用户密钥，实现正确调用。</p><p>l 如果成功，更新前端全局状态并提示“模型切换成功”；如果失败，提示“模型切换失败”。</p><p><strong>接口设计：</strong></p><table><thead><tr><th>URL: GET &#x2F;api&#x2F;get_model</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>说明</td></tr><tr><td>响应：</td><td>model</td><td>String</td><td>当前正在使用的模型</td></tr></tbody></table><table><thead><tr><th>URL: POST&#x2F;api&#x2F;set_model</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>必填</td><td>说明</td></tr><tr><td>请求：</td><td>model</td><td>String</td><td>是</td><td>目标模型名称</td></tr><tr><td>响应：</td><td>message</td><td>String</td><td></td><td>操作结果提示，例如“模型切换成功”</td></tr><tr><td></td><td>error</td><td>String</td><td></td><td>错误信息（仅失败时返回）</td></tr></tbody></table><h2 id="3-5参数管理模块"><a href="#3-5参数管理模块" class="headerlink" title="3.5参数管理模块"></a>3.5参数管理模块</h2><p><strong>功能描述</strong>：用户可调整推理参数，包括：temperature、top_p、top_k、max_length，控制模型的输出效果。</p><p><strong>实现细节</strong>：</p><p>（1）配置参数</p><p>l 前端页面加载时（onMounted），向后端请求 &#x2F;api&#x2F;get_parameters；</p><p>l 后端从 chatbot.parameters 中取出当前参数，将其打包为 JSON 返回；</p><p>l 前端逐字段写入 settingStore，避免覆盖本地已有的持久化字段。</p><p>（2）修改参数</p><p>l 用户在界面上调整参数后，调用 sendParameters()，向后端请求&#x2F;api&#x2F;change_parameters；</p><p>l 后端调用 chatbot.set_parameters(…)，依次更新 temperature、top_p、max_length、top_k等，并返回操作信息；</p><p>l 成功时提示“参数应用成功”，失败时提示“发送参数时出错，请重试”。</p><p><strong>接口设计：</strong></p><table><thead><tr><th>URL: GET &#x2F;api&#x2F;get_ parameters</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>说明</td></tr><tr><td>响应：</td><td>temperature</td><td>Float</td><td>采样温度，控制多样性</td></tr><tr><td></td><td>top_k</td><td>Integer</td><td>Top-K采样值</td></tr><tr><td></td><td>top_p</td><td>Float</td><td>Top-P 采样概率</td></tr><tr><td></td><td>max_length</td><td>Integer</td><td>最大生成长度</td></tr></tbody></table><table><thead><tr><th>URL: POST&#x2F;api&#x2F;change_ parameters</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>字段名</td><td>类型</td><td>必填</td><td>说明</td></tr><tr><td>请求：</td><td>temperature</td><td>Float</td><td>否</td><td>采样温度，控制多样性，数值越大越随机</td></tr><tr><td></td><td>top_k</td><td>Integer</td><td>否</td><td>Top-K采样值</td></tr><tr><td></td><td>top_p</td><td>Float</td><td>否</td><td>Top-P 采样概率</td></tr><tr><td></td><td>max_length</td><td>Integer</td><td>否</td><td>最大生成长度</td></tr><tr><td>响应：</td><td>message</td><td>String</td><td></td><td>操作结果提示</td></tr></tbody></table><h2 id="3-6用户界面模块"><a href="#3-6用户界面模块" class="headerlink" title="3.6用户界面模块"></a>3.6用户界面模块</h2><p><strong>功能描述</strong>：提供图形化人机交互界面。</p><p><strong>组件设计：</strong></p><p>（1）登录窗口: 账号、密码输入框。</p><p>（2）主聊天窗口:</p><p>l 侧边栏: 会话列表、“新建会话”、“删除会话”按钮。</p><p>l 主区域: 对话显示区域（带时间戳）、输入框（支持文本和文件上传）、发送按钮。</p><p>l 顶部工具栏: 模型选择框、设置界面展开按钮（设置界面包括参数设置框、导出历史消息按钮）。</p><p>（3）参数设置框: 滑块或输入框用于调整temperature、top_p、max_length等参数。</p><p>（4）历史消息下载：提供选项，包括：导出JSON格式历史消息、导出markdown格式历史消息。</p><h1 id="四、数据设计"><a href="#四、数据设计" class="headerlink" title="四、数据设计"></a>四、数据设计</h1><p>本章节旨在详细定义系统中数据的核心形态、组织结构、生命周期与流转路径。作为一款强调隐私与独立的私有化部署应用，系统在设计上刻意避免了对传统关系型数据库的长期依赖。运行时的核心数据动态存在于应用程序的内存空间中，并主要通过精心设计的前端状态容器与后端业务对象进行管理与交互。系统的持久化机制体现为两种形式：一是启动时读取的静态配置文件，二是由用户主动触发的对话历史导出文件。本次设计的核心演进在于引入 Redis 作为高性能的内存数据存储，专门用于负责聊天历史消息的持久化缓存，这解决了纯内存存储无法在服务重启后保留上下文，以及难以支撑海量历史数据的问题。整体数据设计将紧密围绕前端 Pinia Store、后端业务状态与 Redis 缓存三者之间的高效、一致同步来展开，并对关键业务场景下的数据流动细节进行深入阐述。</p><h2 id="4-1-运行时数据结构"><a href="#4-1-运行时数据结构" class="headerlink" title="4.1 运行时数据结构"></a>4.1 运行时数据结构</h2><p>系统的运行时数据是一个由前端状态、后端业务状态与 Redis 缓存共同构成的协同体系，通过清晰的 RESTful API 契约进行通信和状态同步。</p><p><strong>前端状态管理</strong> <strong>(基于Pinia)</strong> </p><p>前端采用 Vue 3 的组合式 API 与 Pinia 状态管理库，通过模块化、响应式的 Store 来构建应用的客户端数据模型。这些 Store 不仅是 UI 渲染的数据源，也负责封装与用户交互相关的业务逻辑。</p><p>（1）聊天会话存储 (ChatStore): 此 Store 是前端交互状态的核心，管理着用户所有的对话会话及其消息流。它内部维护着一个响应式的会话列表，其中每个会话对象都包含由 nanoid 生成的唯一标识符、一个可读的标题以及构成该对话的所有消息数组。通过一个指向当前活动会话 ID 的响应式引用，以及基于此 ID 派生的计算属性，视图可以轻松获取并渲染当前会话的完整内容。该 Store 提供了一组原子操作方法，例如创建新会话、在会话间切换、向特定会话添加或删除消息，以及删除整个会话。一个关键特性是，该 Store 的所有状态都被配置为持久化，利用浏览器的 localStorage 机制，确保在页面意外刷新或重新打开后，用户的会话列表和消息内容能够得以恢复，提供了连续的用户体验。</p><p>（2）模型与参数存储 (ModelStore &amp; SettingStore): 为了模块化分离关注点，模型选择与生成参数的管理被分配至独立的 Store。ModelStore 负责跟踪当前用户选中的大语言模型标识符（如 “qwen”），而 SettingStore 则管理着一组影响模型生成行为的参数，例如控制创造性的 temperature、影响采样范围的 top_p 和 top_k，以及决定生成长度的 max_length。用户通过下拉框或滑动条进行的调整会即时更新这些 Store 中的状态。同样，这些偏好设置也进行了持久化，保证用户下一次访问应用时，其熟悉的模型和参数配置能够自动生效。</p><p><strong>后端业务状态与</strong> <strong>Redis</strong> <strong>缓存</strong></p><p>后端采用面向对象的设计，通过 Python 类来封装核心业务逻辑和状态。与前端 Store 对应，后端维护着服务端的权威状态。</p><p>（1）核心业务状态: 后端的核心状态由诸如 ModelManager 这样的管理器类持有。它记录了当前系统活跃的模型标识、全局的模型生成参数配置等元信息。这些信息在应用启动时从配置文件加载，并在运行中通过 API 接收前端的更新。</p><p>（2）历史消息的 Redis 缓存策略: 这是数据设计的重大增强。系统不再将用户的历史对话消息完全保存在后端进程的内存变量中，而是引入 Redis 作为专门的、持久化的历史消息存储层。每一个独立的聊天会话在 Redis 中都有一个唯一的键与之对应，该键通常由用户标识与会话 ID 组合而成，其值是一个列表（List）或有序集合（Sorted Set），按序存储着该会话中所有交互消息的 JSON 序列化字符串。这种设计带来了多重优势：首先，它确保了对话历史在应用程序重启后不会丢失，实现了会话的长期连续性；其次，它将海量的历史数据从应用服务器内存中剥离，显著降低了后端的内存压力，提升了系统稳定性；最后，Redis 卓越的读写性能保障了在多次长对话场景下，历史上下文的拼接与检索速度极快，几乎不会成为性能瓶颈。</p><h2 id="4-2-关键业务场景数据流"><a href="#4-2-关键业务场景数据流" class="headerlink" title="4.2 关键业务场景数据流"></a>4.2 关键业务场景数据流</h2><p>以下描述几个核心用户操作背后，数据在前后端及 Redis 之间的完整流动过程：</p><p>（1）用户发送一条新消息：</p><p><strong>前端发起</strong>:用户在界面输入消息并点击发送。ChatStore 中的动作方法会首先在本地 optimistic 地添加一条状态为“发送中”的用户消息到当前会话，以提供即时反馈。随后，通过 Axios 将消息内容、当前会话 ID 以及当前的模型和参数设置封装成 JSON，发送至后端 &#x2F;api&#x2F;chat 端点。</p><p><strong>后端处理与 Redis 交互:</strong> Flask 路由接收到请求。</p><p>a. 历史获取: 根据请求中的会话 ID，后端服务会首先访问 Redis，读取该会话对应的完整历史消息列表，并将其反序列化。</p><p>b. 上下文组装: 将新的用户消息追加到检索到的历史列表中，形成本次模型调用所需的完整上下文提示（Prompt）。</p><p>c. 模型推理: 调用 ModelManager 的统一接口，将组装好的上下文和参数传递给指定的大语言模型进行文本生成。</p><p>d. 历史回写与响应: 模型生成完成后，后端将新产生的助手回复追加到历史列表中，并立即将更新后的整个历史列表序列化后写回 Redis 中的对应键，完成持久化。最后，将助手生成的内容封装成 API 响应返回给前端。</p><p><strong>前端更新</strong>: 前端收到成功响应后，将本地之前添加的“发送中”消息更新为正式消息，并紧接着将返回的助手回复添加到当前会话的消息列表中，完成 UI 的最终渲染。</p><p>（2）用户切换对话模型：</p><p>a. 用户在前端下拉菜单中选择一个新模型（如从 “qwen” 切换为 “baichuan”）。</p><p>b. ModelStore 中的 currentModel 状态立即更新，并通过持久化插件保存到 localStorage。</p><p>c. 此状态更改是全局性的。它不会影响 Redis 中已存储的任何历史消息内容，也不会追溯改变已生成的消息。然而，自此之后，任何新发起的 &#x2F;api&#x2F;chat 请求都会携带这个新的模型标识符，后端将据此加载和调用对应的模型进行后续生成，从而实现模型在会话中的动态切换。</p><p>（3）应用启动与状态同步：</p><p>a. 当用户首次在浏览器中访问应用或刷新页面时，前端 Pinia Store 会从 localStorage 自动水合（hydrate）其状态，恢复出会话列表、上次使用的模型和参数。</p><p>b. 同时，前端应用在初始化阶段（如在根组件的 onMounted 钩子中）会向后端发起一个轻量的初始化请求（例如 GET &#x2F;api&#x2F;init）。</p><p>c. 后端在该接口中，可以返回系统层面的最新配置或状态（例如默认模型是否有更新），前端据此对本地 Store 进行必要的校正，确保客户端状态与服务器端权威配置保持一致。</p><h2 id="4-3-数据持久化与同步策略"><a href="#4-3-数据持久化与同步策略" class="headerlink" title="4.3 数据持久化与同步策略"></a>4.3 数据持久化与同步策略</h2><p>系统的数据持久化分为三个层次，共同保障数据的可靠性与用户操作的连续性：</p><p><strong>Redis</strong> <strong>持久化</strong> **(对话历史)：**作为对话历史的核心存储，Redis 可以配置为将内存数据定期快照或追加日志到磁盘，确保即使服务重启，所有聊天记录也能完好无损。这是保证“会话连续性”这一核心用户体验的基石。</p><p><strong>浏览器本地存储</strong> <strong>(前端状态)</strong>：Pinia Store 的持久化插件将用户的界面状态（如打开的会话标签、选中的模型、调整的参数）保存在浏览器的 localStorage 中。这属于“用户偏好”的持久化，主要服务于单设备上的体验连贯性，与 Redis 中存储的实质性对话内容互为补充。</p><p><strong>配置文件与用户导出文件</strong> <strong>(静态持久化)：</strong></p><p>（1）配置文件：以 YAML 或 JSON 格式存在于部署目录，存储应用启动所需的基础配置，如 Redis 连接参数、初始模型路径、管理员凭证的哈希值等。它在应用启动时被一次性读入内存。</p><p>（2）用户导出文件：用户通过前端的导出功能，可以将会话历史从当前 Pinia Store 或直接通过 API 从 Redis 中提取的数据，生成为 Markdown 或 JSON 格式的文件并下载到本地。这是用户自主控制的、脱离系统的长期存档方式。</p><p>状态同步：在本设计中，前端 Pinia Store 被视为“视图模型”或“用户交互状态的缓存”，其核心目标是保证 UI 的快速响应和流畅交互。后端的业务对象与 Redis 共同构成了系统数据的“单一可信源”。任何意图改变持久化状态的操作（如发送消息、创建会话），都必须通过后端 API 来执行，由后端负责原子性地更新 Redis 及内部状态，并将结果返回，前端再据此乐观地更新本地 Store。这种模式有效避免了数据不一致和状态冲突。</p>]]></content>
    
    
    <summary type="html">详细设计：使用Flask+VUE搭建本地大语言模型</summary>
    
    
    
    <category term="project" scheme="http://example.com/categories/project/"/>
    
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
    <category term="Interactive system" scheme="http://example.com/tags/Interactive-system/"/>
    
  </entry>
  
  <entry>
    <title>local_llm（用户操作手册）</title>
    <link href="http://example.com/2025/12/20/local-llm/"/>
    <id>http://example.com/2025/12/20/local-llm/</id>
    <published>2025-12-20T07:04:14.000Z</published>
    <updated>2025-12-20T09:10:35.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>本用户操作手册的编写目的是为“大语言模型用户交互工具”的使用者提供一份完整、清晰且可操作的参考文档。通过阅读本手册，读者无需了解底层实现细节，即可理解系统的定位与功能，掌握从启动系统到完成日常智能对话、文档问答与结果导出的基本操作流程。手册既服务于普通业务用户，也面向负责部署和维护系统的技术人员与运维人员，希望在实际使用中为减少沟通成本、提高使用效率、保障系统稳定运行提供统一的依据。</p><p>本手册主要面向四类读者：日常使用系统进行对话和文档分析的业务用户，需要在已有功能基础上继续开发或调优模型调用逻辑的技术研发人员，承担安装部署、监控维护和问题处理任务的系统管理员与运维人员，以及需要根据系统功能与操作流程设计测试用例的测试与质量保障人员。普通用户更关注“如何登录、如何选择模型、如何上传文档并获得回答”等操作性内容；技术人员和管理员则在此基础上，还会结合部署章节和设计类文档，关注运行环境、部署方式和系统结构。</p><p>“大语言模型用户交互工具”本身是一个基于 Vue 3 前端和 Python Flask 后端构建的本地化大模型交互系统，支持私有化部署，能够在局域网甚至离线环境下运行。系统集成了多种开源大语言模型，例如 Qwen、Baichuan、Yi、Mistral、Zephyr 以及九格等，用户可以在统一界面中自由切换模型，并通过多轮对话、文档上传和参数调节完成各类智能问答任务。与依赖外部云端 API 的在线服务相比，本系统更强调数据安全和可控性，适合对隐私与合规要求较高的应用场景。</p><p>本手册的整体结构与系统生命周期相对应：第一章对编写目的、读者对象以及系统整体定位进行概述；第二章从用户视角介绍系统背景、功能和整体架构，使读者对“这是什么系统、能做什么、大致怎么运行”有一个清晰印象；第三章重点介绍系统的部署方式和软硬件环境要求，说明如何在本地环境和 Docker 环境中将系统正确运行起来。第四章主要介绍用户操作此系统的详细步骤。</p><h1 id="2-系统介绍"><a href="#2-系统介绍" class="headerlink" title="2. 系统介绍"></a>2. 系统介绍</h1><p>本章从使用者的角度，对“大语言模型用户交互工具”的背景、设计目标、主要功能和整体运行方式进行介绍，帮助读者在不接触源码的前提下理解系统是什么、解决什么问题、具备哪些能力以及大致如何工作。</p><p>在项目背景方面，系统诞生于当前企业和科研机构对数据安全与隐私保护日益重视的环境之下。许多组织希望充分利用大语言模型的强大能力，却又不希望将业务数据和敏感文档上传到外部公有云服务。针对这一矛盾，本系统选择基于本地可部署的开源模型构建统一的交互平台。系统的目标是，在兼顾安全与性能的前提下，为用户提供一个可以在 Windows 和 Linux 平台上稳定运行、可在本地或局域网内部署、可灵活接入多种开源模型的统一交互工具，满足日常问答、文档分析与辅助创作等多种使用需求。</p><p>从功能角度看，系统围绕“多模型聊天”“文档问答”“会话管理”“参数控制”和“历史导出”等核心场景进行设计。用户首先通过登录页面进入系统主界面，在一个类似即时通讯的对话窗口中与选定的大语言模型进行自然语言对话。系统预先集成了 Qwen、Baichuan、Yi、Mistral、Zephyr 和九格等至少六种、参数规模不低于 7B 的开源大模型，用户可以根据任务偏好在界面中随时切换当前使用的模型，不同模型的回答风格和擅长领域也各不相同。为了更好地服务多轮对话场景，系统提供会话管理功能，用户可以为不同话题分别建立对话会话，查看和切换历史会话，也可以删除不再需要的记录。除了直接输入问题外，系统还支持上传文本文档等文件，自动抽取文本内容并将其纳入当前会话上下文，用户可以围绕文档内容进行摘要、问答或条目提取等操作。对于模型生成行为，系统提供温度、top_p、top_k、最大生成长度等参数调节入口，方便用户在“更稳定严谨”与“更有创造性”的输出风格之间进行调节。当对话内容需要保存和归档时，系统还支持将某个会话导出为文件，便于后续整理、分享或溯源。</p><p>在系统架构与运行模式方面，本系统采用前后端分离的设计思路，前端使用 Vue 3 及相关组件库构建单页面应用，通过浏览器呈现聊天界面、会话列表、模型选择与参数设置等交互元素，负责接收用户输入、触发操作并展示结果。后端基于 Python Flask 实现 Web 服务接口，负责接收前端请求、校验参数、组织上下文并调用模型推理逻辑。模型推理部分通过深度学习框架加载本地大语言模型权重，按当前用户选择的模型和参数生成回答，同时可以利用 Redis 等缓存组件加速对话历史的读取和上下文管理。整体来看，用户在浏览器中的每一次发送操作，背后都由前端组装请求并调用后端接口，后端再调度对应的模型执行推理并将生成结果返回，最终在界面上以对话气泡的形式呈现。</p><p>在运行平台与部署方式上，系统兼容 Windows 和 Linux 两类常用操作系统，提供本地部署和 Docker 部署两条等价的路径。对于倾向于直接在本机调试和开发的用户，可以在安装 Conda 等工具后通过 environment.yml 创建 Python 环境，下载六个模型文件到指定目录，使用 python app.py 启动服务，并在浏览器访问 <a href="http://127.0.0.1:5000。">http://127.0.0.1:5000。</a> 对于更关注环境一致性和跨机器迁移的用户，可以选择 Docker 方案：在目标机器上安装 Docker 和 Docker Compose，准备好 Redis 服务和模型文件目录后，通过 docker compose up -d –build 一键启动所有服务，同样通过浏览器访问同一地址即可。第三章对这两种部署方式的软硬件要求和操作步骤作了详细说明，普通用户通常只需要在系统已经部署完成后，记住访问地址和基本使用流程即可。</p><p>从典型使用过程来看，用户在系统部署完成后，只需打开浏览器、访问系统地址并完成登录，就可以在主界面中选择希望使用的大语言模型，按需调节生成参数，然后像与智能助手聊天一样提出问题、粘贴文本或上传文件。系统会自动记录和展示对话历史，支持在多个会话和不同模型之间自由切换。在此基础上，用户不必关心底层模型加载和资源调度的具体细节，便可以在一个统一、安全、可控的环境中持续利用本地大语言模型的能力开展日常工作与研究探索。</p><h1 id="3-系统部署"><a href="#3-系统部署" class="headerlink" title="3. 系统部署"></a>3. 系统部署</h1><p>本节对系统在不同环境下的部署方法进行详细说明，包括软硬件环境要求、本地部署流程以及基于 Docker 的容器化部署流程。系统当前支持两种等价的部署方式：<br> （1）基于 Conda 的本地部署；（2）基于 Docker 的容器化部署。</p><p>无论采用哪一种方式，系统最终均通过浏览器访问地址 <a href="http://127.0.0.1:5000/">http://127.0.0.1:5000</a> 提供服务。当前实现中未使用环境变量进行配置，涉及模型路径、服务端口与 Redis 连接信息等均在代码或配置文件中固定给出，因此部署过程相对稳定、可控。</p><h2 id="3-1部署架构与运行模式概述"><a href="#3-1部署架构与运行模式概述" class="headerlink" title="3.1部署架构与运行模式概述"></a>3.1部署架构与运行模式概述</h2><p>系统整体上可视为一个提供推理服务的 Web 应用：后端负责加载六个预训练模型并对输入数据进行处理，前端通过 HTTP 接口与后端交互。</p><p>在本地部署模式下，所有组件直接运行在宿主操作系统之上，开发者通过 Conda 管理 Python 环境，使用 python app.py 启动服务即可。该方式便于调试和快速迭代代码，适合作为研究与开发阶段的主要运行模式。</p><p>在 Docker 部署模式下，应用运行所需的运行时环境、依赖库和代码被打包进 Docker 镜像中，用户仅需准备 Docker 服务、Redis 服务以及必要的模型文件，即可通过 docker compose up -d –build 启动系统。该方式具有更好的可移植性与环境一致性，适合在不同物理机器、不同操作系统上进行复现与部署。</p><h2 id="3-2-系统软硬件环境要求"><a href="#3-2-系统软硬件环境要求" class="headerlink" title="3.2 系统软硬件环境要求"></a>3.2 系统软硬件环境要求</h2><p><strong>硬件环境：</strong></p><p>系统主要执行模型推理与轻量级 Web 服务，对 CPU、内存和存储空间有一定要求。结合当前实验与测试情况，建议的硬件配置如下：</p><p>处理器：建议使用 64 位多核 CPU，推荐不少于 4 个物理核心，以保障模型加载及并发请求时的响应速度。</p><p>内存：建议不低于 16 GB。系统需同时加载六个预训练模型，较大的内存可以有效减少频繁的模型卸载与重加载，保证推理过程稳定。</p><p>存储空间：需预留足够磁盘空间用于存放六个模型文件、日志文件及中间结果。根据模型大小，建议专门为模型目录预留至少数十 GB 的可用空间，并预留额外空间用于后续扩展。</p><p>GPU：若部署环境中配备支持主流深度学习框架的 GPU（如支持 CUDA 的显卡），可在本地部署模式下启用 GPU 推理，进一步降低响应时延、提升系统吞吐量。系统未强制依赖 GPU，在仅有 CPU 的环境中仍可正常运行。</p><p>对于仅用于功能验证或小规模测试的桌面环境，可适当降低配置要求，但需注意模型加载时间与推理耗时可能增加。</p><p><strong>软件环境：</strong></p><p>根据不同部署方式，软件环境要求有所差异，但均基于当前主流系统与工具。</p><p>（1）本地部署所需软件环境包括：</p><p>l 操作系统：支持 64 位的主流操作系统，如 Ubuntu 等 Linux 发行版，或 Windows 10&#x2F;11 等 64 位桌面系统。</p><p>l Python 运行环境：需安装 Anaconda 或 Miniconda，用于基于 environment.yml 创建隔离的 Python 环境。具体 Python 版本由 environment.yml 文件指定。</p><p>l 依赖库：通过 environment.yml 自动安装，包括深度学习框架、Web 框架以及与 Redis 通信的相关库等，无需手动逐项安装。</p><p>（2）Docker 部署所需软件环境包括：</p><p>l Docker 服务：需要安装并启用 Docker。在 Windows 平台通常使用 Docker Desktop，在 Linux 平台可直接安装 Docker Engine。</p><p>l Docker Compose：需要安装 Docker Compose，用于解析和执行 docker-compose.yml，管理多个容器服务的协同运行。</p><p>l Redis 服务：系统运行依赖 Redis，用于缓存、任务队列或状态管理。部署时需要确保存在可用的 Redis 实例，并与应用中约定的地址和端口保持一致。Redis 可以部署在宿主机上，也可以以单独的容器形式运行。</p><p>系统当前未通过环境变量进行配置，因此 Redis 连接参数和服务端口等在代码或配置文件中固定，如需调整需修改相应配置并重新部署。</p><h2 id="3-3部署流程"><a href="#3-3部署流程" class="headerlink" title="3.3部署流程"></a>3.3部署流程</h2><p><strong>Conda部署方案：</strong></p><p>本地部署方案为开发与调试提供了良好的透明性，便于研究者在代码层面进行修改和测试。整体流程可分为代码获取、模型准备、环境创建与服务启动几部分。</p><p>首先需要将系统源代码获取到本地，例如通过版本控制系统克隆项目仓库，或直接下载安装包后解压。在项目根目录下应包含主程序 app.py、环境描述文件 environment.yml 以及用于存放模型的目录或路径配置。</p><p>随后需要准备六个预训练模型文件。部署前应将模型下载或拷贝至系统约定的模型目录中，例如项目根目录下的 models&#x2F; 文件夹，或者其他在代码中指定的路径。由于系统未使用环境变量配置模型路径，因此该步骤需严格按照代码中约定的目录结构进行操作，确保运行时能够正确定位并加载模型。</p><p>环境创建基于 environment.yml 完成。在目标机器上安装好 Anaconda 或 Miniconda 后，可在项目根目录执行类似命令：</p><p>conda env create -f environment.yml</p><p>conda activate backend</p><p>环境创建完成后，所有运行系统所需的依赖库均会安装在该虚拟环境中。该方式确保了不同机器上依赖版本的一致性，便于实验结果的复现。</p><p>在完成模型准备与环境创建后，用户可以在同一目录下启动服务：</p><p>python app.py</p><p>程序启动成功后，系统默认在本地 5000 端口监听请求。此时可在浏览器地址栏输入：</p><p><a href="http://127.0.0.1:5000/">http://127.0.0.1:5000</a></p><p>即可访问系统界面并进行交互。如果需要在局域网中为其他终端提供访问，可以将服务绑定地址改为 0.0.0.0，并在操作系统防火墙或网关设备上放行对应端口。</p><p>在本地部署过程中，若出现依赖安装失败、模型路径错误或 Redis 连接异常等问题，可首先检查 environment.yml 是否成功执行、模型目录是否完整、Redis 服务是否正常运行，再结合控制台输出日志进行排查。</p><p><strong>Docker</strong> <strong>部署方案</strong>:</p><p>Docker 部署方案通过容器技术将运行时环境与应用代码统一封装，适合在多台机器或异构操作系统之间迁移与部署同一套系统。其核心思想是：在一个标准化的镜像中包含系统所需的操作系统依赖、Python 运行时及相关库，用户仅需拉取或构建镜像并启动容器即可。</p><p>在部署前，需要确保目标机器已正确安装 Docker 与 Docker Compose，并确保 Redis 服务可用。Windows 环境可通过 Docker Desktop 提供的 Linux 容器模式运行本系统；Linux 环境则可直接依靠系统服务运行 Docker。两类平台在使用方式上基本一致，仅在宿主机目录路径的书写形式上略有差异。</p><p>模型文件的准备方式与本地部署基本相同，同样需要将六个预训练模型下载或拷贝到宿主机指定的目录。该目录会通过 docker-compose.yml 中的卷挂载机制映射到容器内部的固定路径，例如将宿主机的 .&#x2F;models 映射为容器内的 &#x2F;models。应用在容器中加载模型时，只需访问容器内部的路径；对于部署者而言，只需保证宿主机的模型目录结构与 docker-compose.yml 中的配置保持一致即可。</p><p>在完成上述准备后，用户可以在项目根目录使用 Docker Compose 启动系统：</p><p>docker compose up -d –build</p><p>该命令会在必要时根据 Dockerfile 构建应用镜像，并以后台模式启动所有在 docker-compose.yml 中定义的服务（包括核心应用服务以及可能存在的辅助服务）。待容器启动完成后，系统同样在宿主机的 5000 端口提供 Web 服务，可以通过 <a href="http://127.0.0.1:5000/">http://127.0.0.1:5000</a> 进行访问。若部署在远程服务器上，可在浏览器中将 127.0.0.1 替换为服务器的 IP 地址或域名。</p><p>在 Docker 部署模式下，Windows 与 Linux 平台均可直接使用上述命令完成部署。Windows 环境下，Docker Desktop 默认运行 Linux 容器，应用镜像与 Linux 环境保持一致；Linux 环境下则无需额外虚拟化层。这样，可以在不修改应用代码与配置的前提下，保证不同平台上的系统行为保持一致。</p><p>为便于日常运维与问题排查，可以使用 Docker 提供的日志与状态查询命令。例如：</p><p>docker compose ps</p><p>docker compose logs -f</p><p>前者用于查看各服务容器的运行状态，后者用于实时查看日志输出，有助于定位模型加载失败、Redis 连接异常等运行时问题。如需停止服务，可执行：</p><p>docker compose down</p><p>释放相关容器资源。</p><p>需要强调的是，当前 Docker 部署中同样未使用环境变量，Redis 地址、模型路径和服务端口等均在 Docker 配置和应用代码中固定。若需在实际部署中调整端口映射或修改 Redis 服务地址，可在 docker-compose.yml 或应用配置文件中进行相应修改，然后重新构建镜像或重启服务。</p><h2 id="3-4常见问题与部署建议"><a href="#3-4常见问题与部署建议" class="headerlink" title="3.4常见问题与部署建议"></a>3.4常见问题与部署建议</h2><p>在实际部署过程中，常见问题多集中于模型路径错误、依赖环境不完整以及 Redis 服务配置不当。对于本地部署，如遇到模型无法加载的问题，可优先检查模型目录下文件是否齐全、文件名是否与代码约定一致。对于 Docker 部署，则需同时核对宿主机模型目录与容器中挂载路径之间的映射关系。</p><p>在 Redis 相关问题方面，应确认 Redis 服务已启动且监听在正确端口，并与应用中写定的地址一致。如在 Docker 环境中使用单独的 Redis 容器，需要根据容器网络配置调整连接地址。</p><p>建议阅读项目中的<strong>README</strong> 文件，执行示例命令以及必要的脚本。</p><p>综合来看，本系统的部署支持本地环境和 Docker 环境两种路径。本地部署强调对环境与代码的直接掌控，适合开发与调试场景；Docker 部署强调环境的一致性和跨平台可移植性，适合在多台机器、不同操作系统和多人协作的场景下快速复现和上线。</p><h1 id="4-系统各模块使用操作说明"><a href="#4-系统各模块使用操作说明" class="headerlink" title="4.系统各模块使用操作说明"></a>4.系统各模块使用操作说明</h1><p>本章面向所有使用私有化大语言模型交互工具的用户，旨在提供全面、清晰的操作指引。本工具集成了多个先进的开源大语言模型，并提供了直观的图形化界面，使得用户无需具备专业的机器学习知识，即可安全、便捷地利用大语言模型能力进行智能问答、文档分析与知识提炼。</p><h2 id="4-1系统访问与初始认证模块："><a href="#4-1系统访问与初始认证模块：" class="headerlink" title="4.1系统访问与初始认证模块："></a>4.1系统访问与初始认证模块：</h2><p>启动应用程序：用户通过命令启动服务，程序启动后，会自动初始化本地服务并唤起系统默认的网页浏览器，导航至工具的本地址登录页面。</p><p><img src="/../image/clip_image002.png" alt="img"></p><p>图1 登录界面</p><p>完成登录验证：首次使用或访问工具时，浏览器将呈现登录界面。用户需要在登录表单中输入由系统管理员分配的有效账户凭证。账户名填写在“用户名”栏目，对应的密码则需输入到“密码”栏目。默认用户名：admin 默认密码：123</p><p>输入完成后，点击“登录”按钮提交验证。系统后端会将用户提交的密码经过哈希处理后，与预置的安全哈希值进行比对。验证通过后，前端界面会跳转至主工作台，并建立一个本地会话状态，允许用户进行后续操作。若验证失败，页面会明确提示“用户名或密码错误”，用户应检查输入是否正确，注意区分大小写，并确认键盘输入状态。</p><h2 id="4-2主界面布局与功能区域解析："><a href="#4-2主界面布局与功能区域解析：" class="headerlink" title="4.2主界面布局与功能区域解析："></a>4.2主界面布局与功能区域解析：</h2><p>成功登录后，用户将进入应用的主工作界面。整个界面经过精心设计，划分为逻辑清晰、职责明确的三个主要区域，以支持高效的人机交互。</p><p><img src="/../image/clip_image004.png" alt="img"></p><p>图2 主界面布局图</p><p>左侧边栏：会话管理中心，侧边栏是管理所有对话线程的核心面板。顶部显著的“新建会话”按钮用于发起一次全新的对话。下方以列表形式纵向排列所有历史会话条目，每条会话卡片展示着用户自定义的会话标题。当前处于激活状态的会话会通过高亮边框、背景色变化或图标指示等方式进行视觉强调。用户在此区域可以执行会话的创建、选择、删除等生命周期管理操作。</p><p>中央主区域：对话交互与展示区，这是用户与AI进行实质性交流的核心区域。区域主体部分是一个按问答顺序、时间顺序滚动的聊天窗口，清晰展示对话的完整脉络。主区域上方提供简明的智能问答示例，消息以对话形式展现，每一条消息均包含时间戳和消息内容气泡。用户发出的消息与AI模型的回复左右或上下交替排列，形成自然的对话流。区域底部固定附着多功能组合输入框，集成了文本输入和文件上传入口，发送消息操作以回车键触发，是用户发起交互的起点。</p><p>顶部工具栏：全局控制区，位于界面顶部的工具栏提供全局性的控制功能。核心组件包括模型类型选择框，支持选择本地模型和API调用模型两种方式，一个模型选择下拉菜单包括全部已集成的主流模型，用于在集成的多个大语言模型（如Qwen, Baichuan, Yi等）之间进行切换。右侧展示用户头像，可以通过点击头像的“退出”按钮，完成退出操作。</p><p>侧边面板：一个设置图标（齿轮状），用于触发或收起右侧的高级设置面板。该侧边面板展开后，提供对模型生成参数（如Temperature, Top-p, Max Length等）进行精细化调节的滑块或输入控件，方便用户根据任务需求定制AI的响应风格与行为。提供当前会话的消息记录导出按钮，方便用户将对话历史导出为json或markdown格式。</p><p><img src="/../image/clip_image006.png" alt="img"></p><p>图3 设置面板界面图</p><h2 id="4-3核心工作流：发起并管理智能对话："><a href="#4-3核心工作流：发起并管理智能对话：" class="headerlink" title="4.3核心工作流：发起并管理智能对话："></a>4.3核心工作流：发起并管理智能对话：</h2><p><strong>创建与启动新会话</strong>：当用户需要开启一个全新的、独立的话题或任务时，应点击左侧边栏顶部的“新建会话”按钮。系统会瞬时创建一个空的对话容器，并弹出创建会话名称弹窗，新创建的会话会自动成为当前活动会话，输入框随即获得焦点，等待用户输入。</p><p><img src="/../image/clip_image008.png" alt="img"></p><p>图4 创建会话界面图</p><p><strong>执行文本问答交互</strong>：用户在主区域底部的文本输入框中直接键入问题、指令或需要讨论的文本。输入内容可以是一个简单的查询、一段需要润色的文字、一个复杂的分析任务请求，或是任何期望获得AI协助的自然语言表述。输入完成后，可通过按下键盘上的“Enter”键，或直接用鼠标点击输入框右侧的“发送”按钮来提交请求。提交后，用户输入会立即以消息气泡的形式显示在聊天窗口中，并显示为蓝色气泡。同时，系统界面会在输入框上方给出等待状态动画。模型推理完成后，其生成的回复将以“助手”标识出现在用户消息下方，完成一轮交互。用户可以基于此回复进行持续的多轮对话，模型能够有效地保持对当前会话上下文的记忆与理解。</p><p><img src="/../image/clip_image010.png" alt="img"></p><p>图5 文本问答交互效果图</p><p><strong>实现文档分析与问答</strong>：除直接输入文本外，工具支持通过上传本地文档的方式提供更丰富的上下文信息。点击输入框旁的文件上传图标（通常显示为曲别针或文件夹图形），从本地文件系统中选择符合格式要求的文档（当前支持.txt纯文本文件和.md Markdown文件）。</p><p><img src="/../image/clip_image012.png" alt="img"></p><p>图6 上传文件效果图</p><p>文件被选中后，前端会将其上传至后端服务器。后端服务会解析文件内容，提取出全部文本。随后，这份被提取的文本会作为一条特殊的“用户”消息（可能带有文件来源标识）插入到当前对话历史中。此后，用户可以在输入框中针对这份文档内容提出具体问题，例如要求总结、提取关键信息、翻译特定段落或基于内容进行推理。AI模型将依据已提供的完整文档文本来生成相关且准确的答复。</p><h2 id="4-4高级功能应用与个性化配置"><a href="#4-4高级功能应用与个性化配置" class="headerlink" title="4.4高级功能应用与个性化配置"></a>4.4高级功能应用与个性化配置</h2><p><strong>管理多线程对话历史</strong>：工具支持并行的多会话管理能力，允许用户在不同的主题或项目间无缝切换。用户只需在左侧边栏的会话列表中点击任意一个历史会话的标题，主对话区的内容就会立即刷新为该会话的完整历史记录。用户可以在此上下文基础上继续深入交流。当某个会话不再需要时，可将鼠标悬停在该会话卡片上，点击出现的“删除”图标（通常是垃圾桶符号）来永久移除该会话及其所有历史数据。删除操作前可能会有确认提示，且删除当前活动会话时，系统会自动激活另一个现有会话。</p><p><img src="/../image/clip_image013.png" alt="img"></p><p>图7 会话控制界面图</p><p><strong>导出对话记录以备后用</strong>：对于有保留价值的对话，工具支持将会话历史导出为便携式文件。用户需要在目标会话为活动状态时，找到界面中的“导出”功能入口（可能位于会话卡片菜单、顶部工具栏或右键菜单中）。点击后，可选择导出格式：Markdown (.md) 格式适合人类阅读、编辑或在支持Markdown的平台上发布分享；JSON (.json) 格式则完整保留了消息的结构化数据、元信息，适用于程序化分析或作为数据备份。选择格式后，浏览器会自动下载生成的文件到本地默认下载目录。具体操作流程如下图：</p><p><img src="/../image/clip_image014.png" alt="img"></p><p>图8 历史消息下载模块</p><p><img src="/../image/clip_image016.png" alt="img"></p><p>图9 生成下载链接</p><p><img src="/../image/clip_image018.png" alt="img"></p><p>图10 下载完成并打开文件</p><p><strong>动态切换底层大语言模型</strong>：工具预装了多个不同规模与特性的大语言模型。用户可以通过顶部工具栏的下拉菜单，随时选择切换至另一个可用模型。切换动作触发后，界面可能会短暂显示“模型加载中…”的提示，所需时间取决于模型大小和硬件性能。切换成功后，后续所有新的消息生成请求都将由新选的模型处理。需要明确的是，模型切换操作不影响当前会话中已存在的历史消息的显示内容。</p><p><img src="/../image/clip_image020.png" alt="img"></p><p>图11 切换模型界面图</p><p><strong>精细化调控模型生成行为</strong>：为了获得更符合特定任务期望的回复，用户可以调整模型的高级生成参数。通过点击顶部工具栏的设置图标，展开右侧的参数控制面板。</p><p><img src="/../image/clip_image021.png" alt="img"></p><p>主要可调参数包括：</p><p>温度：控制生成文本的随机性。较低的数值使输出更集中、确定和可预测；较高的数值则鼓励更多样化、创造性的表达，但可能降低连贯性。</p><p>最大生成长度：限制模型单次响应所能生成的最大令牌数（大致对应字数），有效防止生成过于冗长或失控的回复。</p><p>Top-p（核采样）：与温度配合使用，这是一种动态选择词汇范围的概率阈值方法，能平衡生成的质量与多样性。</p><p>Top-k：在每一步预测时，仅从概率最高的k个候选词中进行采样，也是一种控制随机性的方法。 用户调整这些滑块或输入框的值后，新的参数设置会即时生效，并应用于此后发起的所有生成请求，无需额外保存。不熟悉这些参数的用户保持默认设置即可获得均衡的体验。</p>]]></content>
    
    
    <summary type="html">用户操作手册：使用Flask+VUE搭建本地大语言模型</summary>
    
    
    
    <category term="project" scheme="http://example.com/categories/project/"/>
    
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
    <category term="Interactive system" scheme="http://example.com/tags/Interactive-system/"/>
    
  </entry>
  
  <entry>
    <title>DYY(三)</title>
    <link href="http://example.com/2025/12/19/2025-12-19-DYY(%E4%B8%89)/"/>
    <id>http://example.com/2025/12/19/2025-12-19-DYY(%E4%B8%89)/</id>
    <published>2025-12-19T10:49:36.000Z</published>
    <updated>2025-12-20T09:42:42.136Z</updated>
    
    <content type="html"><![CDATA[<h3 id="三、指标测试"><a href="#三、指标测试" class="headerlink" title="三、指标测试"></a>三、指标测试</h3><h4 id="（1）指标介绍"><a href="#（1）指标介绍" class="headerlink" title="（1）指标介绍"></a>（1）指标介绍</h4><p>提取准确率：测试FPR&#x2F;TPR</p><p>鲁棒性：测试文本经过修改后的TPR&#x2F;FPR</p><p>生成质量：测试文本的PPL&#x2F;log_diversity&#x2F;BLEU</p><h4 id="（2）核心框架"><a href="#（2）核心框架" class="headerlink" title="（2）核心框架"></a>（2）核心框架</h4><p>批量测试模块是一个<strong>多指标评估框架</strong>，采用<strong>策略模式+管道模式</strong>的混合架构。系统围绕三个核心评估维度（提取准确率、鲁棒性、生成质量）构建，通过统一的调度接口协调不同测试流程，确保评估的一致性和可扩展性。</p><p><img src="/../image/DYY%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E7%BB%98%E5%9B%BE.jpg" alt="DYY代码逻辑绘图"></p><h5 id="统一接口"><a href="#统一接口" class="headerlink" title="统一接口"></a><strong>统一接口</strong></h5><p>generate_batch(水印实例, 数据集, 评估指标):根据指标类型分发到对应处理器：</p><ul><li><strong>提取准确率</strong> → 调用提取准确率模块 [使用管道WatermarkDetectionPipeline]</li><li><strong>鲁棒性</strong> → 调用鲁棒性模块  [使用管道WatermarkDetectionPipeline]</li><li><strong>生成质量</strong> → 调用生成质量模块 [使用管道TextQualityAnalysisPipeline]</li></ul><p>各模块返回：<br>       1. 结果文件（完整测试记录）<br>       2. 指标计算结果（汇总统计）</p><h5 id="管道系统的分层设计"><a href="#管道系统的分层设计" class="headerlink" title="管道系统的分层设计"></a><strong>管道系统的分层设计</strong></h5><p><strong>第一层：抽象管道基类 (BasePipeline)</strong></p><ul><li><strong>职责</strong>：定义评估流程的标准骨架</li><li><strong>核心方法</strong>：<code>evaluate()</code>（模板方法）</li><li><strong>流程控制</strong>：生成（generate） → 攻击（edit） → 检测(detect)&#x2F;分析(analyze) → 记录(result)</li></ul><p><strong>第二层：专用管道实现</strong></p><ol><li><strong>水印检测管道族</strong> (WatermarkDetectionPipeline)<ul><li>共同点：共享检测逻辑 (<code>_detect()</code>)、集成文本编辑器（攻击），专注于文本修改</li><li>差异点：文本生成方式不同<ul><li><code>WatermarkedTextDetectionPipeline</code>：生成带水印文本</li><li><code>UnWatermarkedTextDetectionPipeline</code>：生成普通文本</li></ul></li><li>扩展性：可以组合攻击模块进行攻击后的提取准确率测试（即鲁棒性测试）</li></ul></li><li><strong>质量分析管道</strong> (TextQualityAnalysisPipeline)<ul><li>特点：集成质量分析器，专注文本质量评估</li><li>差异点：评估文本的方式不同<ul><li><code>DirectTextQualityAnalysisPipeline</code>：独立评估生成文本</li><li><code>ReferencedTextQualityAnalysisPipeline</code>: 依据参考文本评估生成文本</li></ul></li><li>扩展性：可组合攻击模块进行对抗性质量测试</li></ul></li></ol><p><strong>第三层：结果封装</strong></p><ul><li><strong>PipelineResult类</strong>：标准化结果数据结构</li><li><strong>包含内容</strong>：生成文本记录、检测结果记录</li><li><strong>设计目的</strong>：确保不同管道输出格式统一</li></ul><h5 id="攻击与质量分析模块的工厂设计"><a href="#攻击与质量分析模块的工厂设计" class="headerlink" title="攻击与质量分析模块的工厂设计"></a>攻击与质量分析模块的工厂设计</h5><p><strong>攻击模块工厂</strong></p><ul><li><strong>输入</strong>：攻击类型名称 + 参数配置</li><li><strong>输出</strong>：具体攻击实例（如改写攻击、增加攻击、删除攻击）</li><li><strong>设计原则</strong>：每种攻击实现统一接口，便于组合测试</li></ul><p><strong>质量分析器工厂</strong></p><ul><li><strong>输入</strong>：质量指标名称 + 评估参数</li><li><strong>输出</strong>：具体分析器实例（如困惑度分析、多样性分析）</li><li><strong>设计原则</strong>：分析器独立，支持多维度质量评估</li></ul><p>这种多模式协同的架构使得批量测试系统既保持了评估流程的规范性，又具备了应对多样化评估需求的灵活性，形成了一个专业、可扩展的水印算法评估平台。</p><h5 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h5><p><img src="/../image/batch_test.png" alt="batch_test"></p>]]></content>
    
    
    <summary type="html">大语言模型水印展示平台</summary>
    
    
    
    <category term="project" scheme="http://example.com/categories/project/"/>
    
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>DYY(二)</title>
    <link href="http://example.com/2025/12/19/DYY2/"/>
    <id>http://example.com/2025/12/19/DYY2/</id>
    <published>2025-12-19T08:49:36.000Z</published>
    <updated>2025-12-20T09:16:50.528Z</updated>
    
    <content type="html"><![CDATA[<h2 id="水印算法集成"><a href="#水印算法集成" class="headerlink" title="水印算法集成"></a>水印算法集成</h2><p><img src="/../image/image-20251219162133738.png" alt="image-20251219162133738"></p><h3 id="1、集成框架"><a href="#1、集成框架" class="headerlink" title="1、集成框架"></a>1、集成框架</h3><p>这是一个基于<strong>工厂模式</strong>+<strong>模板方法模式</strong>+<strong>策略模式</strong> 的大语言模型水印框架。设计上借鉴了Hugging Face Transformers的AutoClass机制，专门针对水印算法场景进行了定制。</p><p>（1）核心组件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 架构组件关系</span></span><br><span class="line">├── config/                          <span class="comment"># JSON配置文件目录</span></span><br><span class="line">│   ├── algorithm_a_config.json</span><br><span class="line">│   └── algorithm_b_config.json</span><br><span class="line">└── watermarks/                      <span class="comment"># 算法目录</span></span><br><span class="line">    ├── algorithm_a/</span><br><span class="line">    │   ├── __init__.py</span><br><span class="line">    │   └── watermark_a.py <span class="comment">#算法实现及参数配置</span></span><br><span class="line">    └── algorithm_b/</span><br><span class="line">    │   ├── __init__.py</span><br><span class="line">    │   └── watermark_b.py</span><br><span class="line">    ...</span><br><span class="line">    ├── auto_config.py<span class="comment"># AutoConfig工厂</span></span><br><span class="line">    ├── auto_watermark.py <span class="comment"># AutoWatermark工厂</span></span><br><span class="line">    └── base.py<span class="comment"># Base基类</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># watermark_a.py:</span></span><br><span class="line">├── <span class="keyword">class</span> <span class="title class_">aConfig</span>(<span class="title class_ inherited__">BaseConfig</span>)</span><br><span class="line">│   ├── <span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">self</span>)</span><br><span class="line">└── <span class="keyword">class</span> <span class="title class_">aWatermark</span>(<span class="title class_ inherited__">BaseWatermark</span>)</span><br><span class="line">    ├── <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">prompt</span>)</span><br><span class="line">    └── <span class="keyword">def</span> <span class="title function_">detect</span>(<span class="params">text</span>)</span><br></pre></td></tr></table></figure><p><img src="/../image/DYY%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E7%BB%98%E5%9B%BE2.jpg" alt="DYY代码逻辑绘图2"><br>（2)<strong>设计特点：</strong></p><ol><li><strong>双重工厂加载机制</strong><ul><li>AutoConfig作为配置工厂，负责加载和初始化算法配置</li><li>AutoWatermark作为算法工厂，负责实例化具体的水印算法</li><li>两者配合实现算法与配置的分离管理</li></ul></li><li><strong>模板方法模式</strong><ul><li>BaseConfig定义配置加载的标准流程</li><li>具体算法通过重写initialize_parameters()实现自定义初始化</li><li>确保所有算法配置具有一致的初始化过程</li></ul></li><li><strong>策略模式</strong><ul><li>BaseWatermark定义水印算法的嵌入与检测（generate&#x2F;detect）</li><li>所有算法重写嵌入与检测过程，满足于算法自身逻辑</li></ul></li><li><strong>插件化算法管理</strong><ul><li>每个算法独立封装为模块，包含Config和Watermark实现</li><li>支持手动注册新算法，扩展性强</li><li>统一的算法接口便于比较和替换不同水印技术</li></ul></li></ol><p>（3）<strong>大致流程：</strong></p><p>a）初始化阶段</p><ul><li><p>用户需要选择某个算法并配置TransformersConfig ，包括：模型&#x2F;分词器&#x2F;生成长度等参数。TransformersConfig需要适配具体的服务场景。</p></li><li><p>调用AutoConfig.load(）执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module = importlib.import_module(module_name)</span><br><span class="line">config_class = <span class="built_in">getattr</span>(module, class_name)</span><br><span class="line">config_instance = config_class(algorithm_config_path, transformers_config)</span><br></pre></td></tr></table></figure><ul><li>根据algorithm_name动态导入对应config模块</li><li>实例化算法特定的Config类(继承BaseConfig)</li><li>BaseConfig.init():<ul><li>保存transformers_config引用</li><li>从config&#x2F;目录加载对应JSON配置文件</li><li>调用self.initialize_parameters() ，其由子类实现</li></ul></li><li>返回完全初始化的Config实例</li></ul></li><li><p>调用AutoWatermark.load() 执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">watermark_config = AutoConfig.load(algorithm_name, transformers_config, algorithm_config_path=algorithm_config)</span><br><span class="line">module = importlib.import_module(module_name)</span><br><span class="line">watermark_class = <span class="built_in">getattr</span>(module, class_name)</span><br><span class="line">watermark_instance = watermark_class(watermark_config)</span><br></pre></td></tr></table></figure><ul><li>根据algorithm_name动态导入算法模块</li><li>实例化算法类，传入Config实例，算法内部实例设置: self.config &#x3D; config</li><li>返回初始化的watermark实例</li></ul></li></ul><p>b)  嵌入与检测阶段</p><ul><li>构建prompt：<br>在具体服务场景下，添加合适的前缀信息。如在翻译场景下，添加：”translate to zh:”等</li><li>使用初始化阶段的watermark实例执行嵌入与检测操作：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">watermarked_text = watermark_instance.generate_watermarked_text(prompt)</span><br><span class="line">detect_result = watermark_instance.detect_watermark(watermarked_text)</span><br></pre></td></tr></table></figure><p>（4）效果展示：</p><p><img src="/../image/image-20251219161834841.png" alt="image-20251219161834841"></p><h3 id="2、算法分类（单比特、多比特、隐写）"><a href="#2、算法分类（单比特、多比特、隐写）" class="headerlink" title="2、算法分类（单比特、多比特、隐写）"></a>2、算法分类（单比特、多比特、隐写）</h3><p><strong>单比特水印算法：</strong></p><table><thead><tr><th>算法名称</th><th>来源</th></tr></thead><tbody><tr><td>KGW</td><td>[<a href="https://arxiv.org/abs/2301.10226">2301.10226] A Watermark for Large Language Models (arxiv.org)</a></td></tr><tr><td>Unigram</td><td>[ <a href="https://arxiv.org/abs/2306.17439">2306.17439] Provable Robust Watermarking for AI-Generated Text (arxiv.org)</a></td></tr><tr><td>EWD</td><td>[<a href="https://arxiv.org/abs/2403.13485">2403.13485] An Entropy-based Text Watermarking Detection Method (arxiv.org)</a></td></tr><tr><td>DIP</td><td>[<a href="https://arxiv.org/abs/2310.07710">2310.07710] A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models (arxiv.org)</a></td></tr><tr><td>Unbiased</td><td>[ <a href="https://arxiv.org/abs/2310.10669">2310.10669] Unbiased Watermark for Large Language Models (arxiv.org)</a></td></tr><tr><td>SynthID</td><td><a href="https://www.nature.com/articles/s41586-024-08025-4">Scalable Watermarking for Identifying Large Language Model Outputs (<em>Nature</em>)</a></td></tr><tr><td>MCMark</td><td><a href="https://www.arxiv.org/pdf/2502.11268">Improved Unbiased Watermark for Large Language Models</a></td></tr><tr><td>SPO</td><td>团队自研（投稿中）</td></tr><tr><td>WR</td><td>团队自研（投稿中）</td></tr></tbody></table><p><strong>多比特水印算法：</strong></p><table><thead><tr><th>算法名称</th><th>来源</th></tr></thead><tbody><tr><td>CycleShift</td><td>[<a href="https://arxiv.org/abs/2308.00113">2308.00113] Three Bricks to Consolidate Watermarks for Large Language Models</a></td></tr><tr><td>SGO</td><td>团队自研（投稿中）</td></tr><tr><td>MEXPEdit</td><td>团队自研（投稿中）</td></tr></tbody></table><p><strong>隐写算法：</strong></p><table><thead><tr><th>算法名称</th><th>来源</th></tr></thead><tbody><tr><td>Arithmetic</td><td>[<a href="https://arxiv.org/abs/1909.01496">1909.01496] Neural Linguistic Steganography</a></td></tr><tr><td>Meteor</td><td><a href="https://dl.acm.org/doi/abs/10.1145/3460120.3484550">https://dl.acm.org/doi/abs/10.1145/3460120.3484550</a>)</td></tr><tr><td>SparSamp<br />团队自研</td><td>[<a href="https://arxiv.org/abs/2503.19499">2503.19499] SparSamp: Efficient Provably Secure Steganography Based on Sparse Sampling</a></td></tr></tbody></table>]]></content>
    
    
    <summary type="html">大语言模型水印展示平台</summary>
    
    
    
    <category term="project" scheme="http://example.com/categories/project/"/>
    
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>DYY(一)</title>
    <link href="http://example.com/2025/12/19/2025-12-19-DYY(%E4%B8%80)/"/>
    <id>http://example.com/2025/12/19/2025-12-19-DYY(%E4%B8%80)/</id>
    <published>2025-12-19T07:49:36.000Z</published>
    <updated>2025-12-20T09:36:30.893Z</updated>
    
    <content type="html"><![CDATA[<p>平台需要兼容四种生成式智能服务，计划采用相应模型适配服务，本文记录调研结果</p><h3 id="1-决策问答生成"><a href="#1-决策问答生成" class="headerlink" title="1. 决策问答生成"></a><strong>1. 决策问答生成</strong></h3><p><strong>核心需求</strong>：理解复杂问题，生成支持决策的答案或分析。</p><table><thead><tr><th align="left">模型类型</th><th align="left">模型名称</th><th align="left">关键特点 &#x2F; 适用场景</th><th align="left">来源&#x2F;地址</th></tr></thead><tbody><tr><td align="left"><strong>通用基础模型</strong></td><td align="left"><strong>T5</strong></td><td align="left">多任务处理能力强，适合问答生成。需大量计算资源。</td><td align="left"><a href="https://github.com/google-research/text-to-text-transfer-transformer">GitHub</a></td></tr><tr><td align="left"></td><td align="left"><strong>BERT</strong></td><td align="left">上下文理解能力强大，但生成能力有限，更适合理解与分类。</td><td align="left"><a href="https://github.com/google-research/bert">GitHub</a></td></tr><tr><td align="left"><strong>Hugging Face 专项模型</strong></td><td align="left"><strong>PlanRAG</strong></td><td align="left">专为决策设计，采用“先规划后检索生成”范式，适合基于数据的复杂决策。</td><td align="left"><a href="https://huggingface.co/papers/2406.12430">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>abhitopia&#x2F;question-answer-generation</strong></td><td align="left">基于T5的多任务模型，同时擅长<strong>问答</strong>和<strong>问题生成</strong>，适应性强。</td><td align="left"><a href="https://huggingface.co/abhitopia/question-answer-generation">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>Generative QA Models</strong></td><td align="left">泛指一类能生成连贯、详细答案的模型，适合需要丰富语境解释的决策场景。</td><td align="left"><a href="https://discuss.huggingface.co/t/selecting-a-chatbot-model-for-question-answering-qa/53120">社区讨论</a></td></tr></tbody></table><hr><h3 id="2-文本摘要生成"><a href="#2-文本摘要生成" class="headerlink" title="2. 文本摘要生成"></a><strong>2. 文本摘要生成</strong></h3><p><strong>核心需求</strong>：从长文本中提取或概括核心信息，生成简洁摘要。</p><table><thead><tr><th align="left">模型类型</th><th align="left">模型名称</th><th align="left">关键特点 &#x2F; 适用场景</th><th align="left">来源&#x2F;地址</th></tr></thead><tbody><tr><td align="left"><strong>通用基础模型</strong></td><td align="left"><strong>BART</strong></td><td align="left">在文本生成和摘要任务中表现优秀，需大量计算资源。</td><td align="left"><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/bart">GitHub</a></td></tr><tr><td align="left"></td><td align="left"><strong>Pegasus</strong></td><td align="left">专为摘要任务设计，性能优越，训练和推理资源需求高。</td><td align="left"><a href="https://github.com/google-research/pegasus">GitHub</a></td></tr><tr><td align="left"><strong>Hugging Face 专项模型</strong></td><td align="left"><strong>facebook&#x2F;bart-large-cnn</strong></td><td align="left">基于BART，在新闻等文本的<strong>摘要生成</strong>上常用且效果良好。</td><td align="left"><a href="https://huggingface.co/facebook/bart-large-cnn">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>pszemraj&#x2F;led-large-book-summary</strong></td><td align="left">基于<strong>LED</strong>架构，专为<strong>处理长文本</strong>（如书籍）摘要设计。</td><td align="left"><a href="https://huggingface.co/pszemraj/led-large-book-summary">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>financial-summarization-pegasus</strong></td><td align="left">基于Pegasus，专门针对<strong>金融新闻和报告</strong>的摘要任务。</td><td align="left"><a href="https://huggingface.co/human-centered-summarization/financial-summarization-pegasus">Hugging Face</a></td></tr></tbody></table><hr><h3 id="3-语言翻译"><a href="#3-语言翻译" class="headerlink" title="3. 语言翻译"></a><strong>3. 语言翻译</strong></h3><p><strong>核心需求</strong>：实现高质量、准确的双语或多语翻译。</p><table><thead><tr><th align="left">模型类型</th><th align="left">模型名称</th><th align="left">关键特点 &#x2F; 适用场景</th><th align="left">来源&#x2F;地址</th></tr></thead><tbody><tr><td align="left"><strong>通用基础模型</strong></td><td align="left"><strong>MarianMT</strong></td><td align="left">多语言支持，翻译质量高，部分语言对需更多数据。</td><td align="left"><a href="https://github.com/Helsinki-NLP/OPUS-MT-train">GitHub</a></td></tr><tr><td align="left"></td><td align="left"><strong>mBART</strong></td><td align="left">多语言支持，强大的生成能力，资源需求较高。</td><td align="left"><a href="https://github.com/pytorch/fairseq/tree/main/examples/mbart">GitHub</a></td></tr><tr><td align="left"><strong>Hugging Face 专项模型</strong></td><td align="left"><strong>opus-mt-zh-en</strong></td><td align="left">MarianMT系列，专门用于<strong>中文到英文</strong>的翻译。</td><td align="left"><a href="https://huggingface.co/Helsinki-NLP/opus-mt-zh-en">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>opus-mt-en-zh</strong></td><td align="left">MarianMT系列，专门用于<strong>英文到中文</strong>的翻译。</td><td align="left"><a href="https://huggingface.co/Helsinki-NLP/opus-mt-en-zh">Hugging Face</a></td></tr></tbody></table><hr><h3 id="4-新闻报道-标题生成"><a href="#4-新闻报道-标题生成" class="headerlink" title="4. 新闻报道&#x2F;标题生成"></a><strong>4. 新闻报道&#x2F;标题生成</strong></h3><p><strong>核心需求</strong>：根据关键信息或内容，生成完整的新闻报道或吸引人的标题。</p><table><thead><tr><th align="left">模型类型</th><th align="left">模型名称</th><th align="left">关键特点 &#x2F; 适用场景</th><th align="left">来源&#x2F;地址</th></tr></thead><tbody><tr><td align="left"><strong>通用基础模型</strong></td><td align="left"><strong>T5</strong></td><td align="left">通用性强，能够生成高质量文本，计算资源需求大。</td><td align="left"><a href="https://github.com/google-research/text-to-text-transfer-transformer">GitHub</a></td></tr><tr><td align="left"></td><td align="left">Qwen2.5</td><td align="left">通用性强，能够生成丰富详细的回答，适用于新闻稿生成。</td><td align="left"><a href="https://huggingface.co/Qwen/Qwen2.5-7B">Qwen&#x2F;Qwen2.5-7B · Hugging Face</a></td></tr><tr><td align="left"><strong>Hugging Face 专项模型</strong></td><td align="left"><strong>czearing&#x2F;article-title-generator</strong></td><td align="left">基于T5，根据文章内容<strong>生成标题</strong>，训练于Medium文章。</td><td align="left"><a href="https://huggingface.co/czearing/article-title-generator">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>fabiochiu&#x2F;t5-base-medium-title-generation</strong></td><td align="left">基于T5，为Medium文章内容<strong>生成标题</strong>。</td><td align="left"><a href="https://huggingface.co/fabiochiu/t5-base-medium-title-generation">Hugging Face</a></td></tr><tr><td align="left"></td><td align="left"><strong>VBART-XLarge-Title-Generation-from-News</strong></td><td align="left">基于VBART，专为<strong>从新闻生成标题</strong>而优化（主要针对土耳其语，架构可参考）。</td><td align="left"><a href="https://huggingface.co/vngrs-ai/VBART-XLarge-Title-Generation-from-News">Hugging Face</a></td></tr></tbody></table><h3 id="数据集收集"><a href="#数据集收集" class="headerlink" title="数据集收集"></a>数据集收集</h3><p>英文数据集：</p><p>新闻 <a href="https://www.kaggle.com/datasets/gpreda/bbc-news">BBC News</a></p><p>翻译 <a href="https://huggingface.co/datasets/FradSer/DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think">FradSer&#x2F;DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think · Datasets at Hugging Face</a></p><p>决策 <a href="https://github.com/yizhongw/self-instruct/tree/main/data">self-instruct&#x2F;data at main · yizhongw&#x2F;self-instruct · GitHub</a></p><p>摘要 <a href="https://github.com/Mzzzhu/CMNEE">https://github.com/Mzzzhu/CMNEE</a></p><p>中文数据集：</p><p>翻译：<a href="https://huggingface.co/datasets/FradSer/DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think">FradSer&#x2F;DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think · Datasets at Hugging Face</a></p><p>决策：<a href="https://huggingface.co/datasets/BAAI/COIG/tree/main">BAAI&#x2F;COIG at main</a></p><p>新闻：<a href="https://github.com/lxj5957/CLTS-Dataset">https://github.com/lxj5957/CLTS-Dataset</a></p><p>军事：<a href="https://github.com/Mzzzhu/CMNEE">https://github.com/Mzzzhu/CMNEE</a></p>]]></content>
    
    
    <summary type="html">大语言模型水印展示平台</summary>
    
    
    
    <category term="project" scheme="http://example.com/categories/project/"/>
    
    
    <category term="LLM" scheme="http://example.com/tags/LLM/"/>
    
  </entry>
  
</feed>
